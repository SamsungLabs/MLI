# ========== LOGGER OPTIONS ==========

visualise_iter: 1000            # How often do you want to display output images during training
validation_iter: 5000
snapshot_save_iter: 5000     # How often do you want to save trained models
log_iter: 20                  # How often do you want to log the training stats
opt_level: O0
use_apex: True

trainer: TrainerSIMPLI
augmentations:
  augs_mix:
    -
      kornia_aug: RandomMixUp
      p: 0.9
      lambda_val: [ 0, 0.4 ]

clip_grad_norm: 20
max_iter: &max_iter 1000000             # maximum number of training iterations
seed: 420
patch_size: 128

weights:
  generator:
    l1: 1.
    perceptual: 0
    intermediate_loss_0_l1: 1.
    intermediate_loss_1_l1: 1.
    intermediate_loss_2_l1: 1.
    intermediate_loss_3_l1: 1.
    smooth_layers: 0.01


# ========== MODEL OPTIONS ==========


models:
  gen:
    architecture: GeneratorSIMPLI
    optimizer_group: generator
    multi_reference_cams: False
    rgb_inter_errors: True
    max_depth: 100
    min_depth: 1
    use_intersected_groups: True
    num_layers: &num_layers 4
    scale_feat_extractor: 1
    scale_lvl0: 0.25
    scale_lvl1: 0.5
    scale_lvl2: 1
    lvl2_error_with_image: True
    lvl3_error_with_image: True
    lvl4_error_with_image: True
    modules:
      features_extractor:
        architecture: SuperPointsFeatExtractor
        half_size_output: True
        input_dim: 3
        output_dim: 9 # 15
        norm: 'ln'
        activation: 'elu'

      sampi_block_lvl0:
        architecture: TBlock
        num_surfaces: 41
        input_size: 12
        input_surfaces_feats_size: 12
        output_surfaces_feats_size: 12
        agg_with_mean_var: True
        post_aggregate_block_conv_dim: 3
        post_aggregate_block_dims: [ [ 32, 32 ], [ 32, 32 ], [ 32, 32 ], [32, 16], [16, 16], [16, 16] ]
      alpha_decoder_lvl0:
        architecture: ConvMLP
        input_dim: 12
        dims: [ 12, 12 ]
        output_dim: 1
        activation: 'elu'

      sampi_block_lvl1:
        architecture: TBlock
        num_surfaces: 41
        input_size: 12
        input_surfaces_feats_size: 13
        output_surfaces_feats_size: 16
        agg_with_mean_var: True
        post_aggregate_block_conv_dim: 2
        post_aggregate_block_dims: [ [ 32, 32 ], [ 32, 16 ] ]
        aggregation_with_weights: True
      planes_deformer_lvl1:
        architecture: AnchoredRayAttention
        dim_input: 16
        num_heads: 2
        dim_hidden: 10
        num_anchors: *num_layers
      depth_decoder_lvl1:
        architecture: ConvMLP
        input_dim: 16
        dims: [ 16, 8 ]
        output_dim: 11 # num_surf // num_layers + 1
        activation: 'relu'
      rgba_decoder_lvl1:
        architecture: ConvMLP
        input_dim: 16
        dims: [ 16, 8 ]
        output_dim: 4
        activation: 'relu'

      samli_block_lvl2:
        architecture: TBlock
        num_surfaces: *num_layers
        input_size: 6
        input_surfaces_feats_size: 16
        output_surfaces_feats_size: 16
        agg_with_mean_var: True
        post_aggregate_block_conv_dim: 2
        post_aggregate_block_dims: [ [ 32, 32 ], [ 16, 16 ] ]
        agg_processor_hidden_size: 16
      rgba_decoder_lvl2:
        architecture: ConvMLP
        input_dim: 16
        dims: [ 16, 8 ]
        output_dim: 4
        activation: 'relu'

      samli_block_lvl3:
        architecture: TBlock
        num_surfaces: *num_layers
        input_size: 6
        input_surfaces_feats_size: 16
        output_surfaces_feats_size: 16
        agg_with_mean_var: True
        post_aggregate_block_conv_dim: 2
        post_aggregate_block_dims: [ [ 16, 16 ] ]
        agg_processor_hidden_size: 16
      rgba_decoder_lvl3:
        architecture: ConvMLP
        input_dim: 16
        dims: [ 16, 8 ]
        output_dim: 4
        activation: 'relu'

      samli_block_lvl4:
        architecture: TBlock
        num_surfaces: *num_layers
        input_size: 6
        input_surfaces_feats_size: 16
        output_surfaces_feats_size: 16
        agg_with_mean_var: True
        post_aggregate_block_conv_dim: 2
        post_aggregate_block_dims: [ [ 16, 16 ] ]
        agg_processor_hidden_size: 16
      rgba_decoder_lvl4:
        architecture: ConvMLP
        input_dim: 16
        dims: [ 16, 8 ]
        output_dim: 4
        activation: 'relu'

      rasterizer:
        architecture: RasterizerFFrast
        faces_per_pixel: 16
        image_size: 256
        scale: 120
      shader:
        architecture: MPIShader
      composer:
        architecture: ComposerStereoMagnification
        black_background: False

optimizers:
  generator:
    type: Adam
    lr: 0.0003
    betas: [0.9, 0.999]

schedulers:
  generator:
    type: ExponentialLR
    gamma: 0.999994



# ========== DATASET OPTIONS ==========

# common part

dataset_commons:
  <<: &dataset_commons
    ref_cameras_number: 1
    novel_cameras_number_per_ref: 4
    relative_intrinsics: True
    image_scale: 1.2

dataset_sword_train:
  <<: &dataset_sword_train
    type: RealEstate2
#    root: <Path to full SWORD dataset https://samsunglabs.github.io/StereoLayers/sword/>
    root: /home/Develop/datasets/Turkish_final_full
    dataset_file: sorted_train.csv
    image_size: [ 360, 640 ]
    max_len_sequence: 35
    cameras_sample_mode: 'randomly'
    set_reference_as_origin: False
    init_cameras_number_per_ref: 2
    transforms_params:
      resize_scales: [ 1.0, 1.2, 1.3, 1.4, 1.5, 1.6 ]
      num_cams_resolution_crop_lensq_weight:
        - [ 2, [ 192, 384 ], [ 192, 384 ], 10, 1 ]
        - [ 3, [ 176, 336 ], [ 176, 336 ], 20, 1 ]
        - [ 4, [ 144, 288 ], [ 144, 288 ], 30, 4 ]
        - [ 6, [ 128, 240 ], [ 128, 240 ], 45, 2 ]
        - [ 8, [ 96, 192 ], [ 96, 192 ], 80, 1 ]
      <<: *dataset_commons
      scale_constant: 1.25

dataset_ipad_val:
  <<: &dataset_ipad_val
    type: RealEstate2
#    root: <Path to ipad sample from Small Test Data https://samsunglabs.github.io/StereoLayers/sword/>
    root: /home/Develop/datasets/sword/AdditionalScenes/partb_ipad
    dataset_file: dataset.csv
    relative_intrinsics: True
    image_size: [ 384, 624 ]
    cameras_sample_mode: 'interpolation_only'
    max_len_sequence: 25
    scale_constant: 2.0
    fix_seed: True
    init_cameras_number_per_ref: 8
    virtual_average_ref_extrinsics: True
    ref_cameras_number: 1


dataloaders:
  train:
    dataset:
       type: JoinedDataset
       weighted_sampling: True
       weights: [1.0]
       concatenated_datasets:
         - <<: *dataset_sword_train
    params:
      shuffle: True
      drop_last: False
      num_workers: 0
      batch_size: 1
      collate_fn_type: without_seqs
      pin_memory: True
  vis:
    dataset:
      <<: *dataset_ipad_val
      novel_cameras_number_per_ref: 1
      virtual_average_ref_extrinsics: True
      sample_first_nrow: 10
    params:
      shuffle: False
      drop_last: True
      num_workers: 1
      batch_size: 1
      pin_memory: True
  val:
    dataset:
      <<: *dataset_ipad_val
      novel_cameras_number_per_ref: 1
    params:
      shuffle: False
      drop_last: False
      num_workers: 1
      batch_size: 1
  render:
    dataset:
      <<: *dataset_ipad_val
      novel_cameras_number_per_ref: 1
      virtual_average_ref_extrinsics: True
      fix_seed: True
    params:
      shuffle: False
      drop_last: False
      num_workers: 1
      batch_size: 1
