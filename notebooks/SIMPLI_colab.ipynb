{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 游릳 SIMPLI colab \n",
        "\n",
        "#### This notebook was composed for demonstrating the SIMPLI model from the paper \"Self-improving Multiplane-to-layer Images for Novel View Synthesis\".\n",
        "#### Model SIMPLI works with a set of scene views, each view is an image with camera parameters. As result, it generates a multiplane layer image (MLI) representation, that could be easily rendered by a standard graphics pipeline to obtain novel scene views.\n",
        "\n",
        "#### This colab includes several steps:\n",
        "\n",
        "\n",
        "1.   Loading user video. \n",
        "2.   Processing frames from video with SFM for obtaining views poses.\n",
        "3.   Selection views that will be used for MLI from processed frames.\n",
        "4.   Build MLI with SIMPLI model.\n",
        "5.   Run online MLI viewer.\n",
        "\n",
        "#### This colab contains parameters that you can adjust, but it works well with default values for videos that have HD resolution and was shot in landscape orientation. 游리 highlights important information.\n",
        "\n",
        "![simpli_scheme](https://github.com/SamsungLabs/MLI/blob/main/docs/resources/simpli_pipeline.jpg?raw=true)\n",
        "\n"
      ],
      "metadata": {
        "id": "TpQwIaSEPNba"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-gejE1oSteY"
      },
      "outputs": [],
      "source": [
        "!pip install -q gwpy       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aFEFK_1D2EK",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title The SIMPLI pipeline in colab needs access to your google drive for storing intermediate results and load modules from GitHub.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "work_dir_path = '/content/gdrive/MyDrive/simpli_test/captures' # @param {type: 'string'}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "FhBSogvRuV91"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjznL7qYDpe5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Instal dependencies\n",
        "%%capture\n",
        "!apt-get install colmap ffmpeg\n",
        "!pip install numpy==1.19.3\n",
        "!pip install p_tqdm \n",
        "!pip install ffmpeg_python\n",
        "!pip3 install torch torchvision torchaudio\n",
        "\n",
        "!apt-get install ffmpeg\n",
        "!pip install Ninja\n",
        "!pip3 install torch torchvision torchaudio\n",
        "%cd \"/content/drive/My Drive/colab\"\n",
        "!git clone https://github.com/NVlabs/nvdiffrast.git\n",
        "%cd \"nvdiffrast\"\n",
        "!pip install .\n",
        "\n",
        "%cd \"..\"\n",
        "\n",
        "!pip install  tqdm \\\n",
        "              h5py \\\n",
        "              scikit-image \\\n",
        "              einops \\\n",
        "              graphviz \\\n",
        "              plyfile \\\n",
        "              munch \\\n",
        "              huepy \\\n",
        "              Cython \\\n",
        "              tensorboardX \\\n",
        "              packaging \\\n",
        "              torchsummary \\\n",
        "              imageio \\\n",
        "              pyyaml \\\n",
        "              opencv-python \\\n",
        "              pyntcloud \\\n",
        "              lpips \\\n",
        "              kornia==0.5.11 \\\n",
        "              ffmpeg_python \\\n",
        "              pandas \\\n",
        "              pillow \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGMN-cUaYOUy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# @title Dependencies imports \n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import json\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from timeit import default_timer as timer\n",
        "import math\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "!mkdir -p /content/gdrive/MyDrive/libs\n",
        "!git clone -b colab https://github.com/palsol/SimpleSfm\n",
        "!rm -rf /content/gdrive/MyDrive/libs/SimpleSfm\n",
        "!mv SimpleSfm /content/gdrive/MyDrive/libs\n",
        "\n",
        "sys.path.append('/content/gdrive/MyDrive/libs/SimpleSfm')\n",
        "\n",
        "from simple_sfm.scene_utils.video_to_scene_processors import OneVideoSceneProcesser\n",
        "from simple_sfm.scene_utils.matcher import Matcher\n",
        "from simple_sfm.scene_utils.colmap_scene_converters import colmap_sparse_to_re10k_like_views\n",
        "from simple_sfm.scene_utils.colmap_bd_utils import ColmapBdManager\n",
        "from simple_sfm.scene_utils.scene_readers import read_re10k_views\n",
        "\n",
        "from simple_sfm.utils.video_streamer import VideoStreamer\n",
        "from simple_sfm.cameras.camera_multiple import CameraMultiple\n",
        "from simple_sfm.cameras.utils import average_extrinsics\n",
        "\n",
        "\n",
        "!git clone https://github.com/SamsungLabs/MLI.git\n",
        "!rm -rf /content/gdrive/MyDrive/libs/MLI\n",
        "!mv MLI /content/gdrive/MyDrive/libs\n",
        "\n",
        "sys.path.append('/content/gdrive/MyDrive/libs/MLI')\n",
        "\n",
        "from lib.trainers.utils import create_trainer_load_weights_from_config\n",
        "from lib.utils.io import get_config\n",
        "\n",
        "\n",
        "# copy viewer extension to nbextensions\n",
        "!rm -rf /usr/local/share/jupyter/nbextensions/google.colab/multi-layer-viewer\n",
        "!cp -R /content/gdrive/MyDrive/libs/MLI/docs/viewer \\\n",
        "    /usr/local/share/jupyter/nbextensions/google.colab/multi-layer-viewer\n",
        "\n",
        "!git clone https://github.com/magicleap/SuperGluePretrainedNetwork.git\n",
        "!rm -rf /content/gdrive/MyDrive/libs/SuperGluePretrainedNetwork\n",
        "!mv SuperGluePretrainedNetwork /content/gdrive/MyDrive/libs\n",
        "\n",
        "\n",
        "superglue_weigths_path = '/content/gdrive/MyDrive/libs/SuperGluePretrainedNetwork/models/weights/superglue_outdoor.pth'\n",
        "superpoint_weigths_path = '/content/gdrive/MyDrive/libs/SuperGluePretrainedNetwork/models/weights/superpoint_v1.pth'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load SIMPLI model.\n",
        "#### 游리 You can select a SIMPLI model that generates 8 or 4 layers MLI representation."
      ],
      "metadata": {
        "id": "xPAQcIkyiIjM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-iUgY3360HW",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_layers = '8' #@param ['4', '8']\n",
        "\n",
        "\n",
        "checkpoints_path = f'/content/gdrive/MyDrive/libs/MLI/pretrained/model{num_layers}_layers/'\n",
        "config_path = f'/content/gdrive/MyDrive/libs/MLI/pretrained/model{num_layers}_layers/tblock{num_layers}.yaml'\n",
        "\n",
        "config = get_config(config_path)\n",
        "iteration = 660000\n",
        "\n",
        "trainer, loaded_iteration = create_trainer_load_weights_from_config(config=config,\n",
        "                                                                    checkpoints_dir=checkpoints_path,\n",
        "                                                                    iteration=iteration,\n",
        "                                                                    device='cuda'\n",
        "                                                                    )\n",
        "\n",
        "_ = trainer.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load your video.\n",
        "#### 游리 For the best result, you need to use a short landscape-oriented video of a static scene. Choose some object and shoot it by moving the camera in front of the object like in the following picture.\n",
        "![How to shoot](https://github.com/SamsungLabs/MLI/blob/main/docs/resources/how_to_record_video_new.jpeg?raw=true)\n",
        "\n",
        "#### 游리 It is better to use a video that has a resolution of ~ 1280x720. If your video is larger, you can set the **scale_factor** parameter in the ***2. Process video with SFM*** section, and it will be resized.\n",
        "#### 游리 You can find a sample video [here](https://github.com/SamsungLabs/MLI/blob/main/docs/resources/sample_video.mp4)\n",
        "#### 游리 Our method is designed for working with static scenes without hard specular effects. However, it could work with scenes that contain a reasonable amount of them.\n",
        "#### 游리 If you want to experiment with other videos without installing dependencies and reloading models you can choose another video and rerun the notebook from this cell. Enjoy!!!\n"
      ],
      "metadata": {
        "id": "uf98Pj4Dic-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zN5Z2kwQkaSH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7ahzAmv9bU8"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7t8ZJ7mmN7N"
      },
      "source": [
        "## 2. Process video with SFM\n",
        "\n",
        "#### 游리 If your video resolution is larger than 1280x720 is recommended to set the **scale_factor**  so that the rescaled video resolution will be approximately 1280x720. Otherwise, the consumption of GPU memory can be greater than available on the collab machine.\n",
        "\n",
        "#### 游리 If your camera shoots video with a significant barrel/pincushion distortion, flag **central_crop** would be helpful.\n",
        "\n",
        "#### 游리 You can increase the maximum number of frames **max_num_frames** that will be processed by the SFM pipeline and will farther be available to select for using in SIMPLI model. However, the execution time of the SFM pipeline could be increased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR1eukT2OWy9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# v c = '/content/gdrive/MyDrive/simpli_test/20220911_161609.mp4'\n",
        "# temp_video_path = '/content/gdrive/MyDrive/simpli_test/2022-08-23_test.mp4'\n",
        "temp_video_path = next(iter(uploaded.keys()))\n",
        "video_full_name = temp_video_path.split('/')[-1]\n",
        "capture_name = video_full_name.split('.')[0]\n",
        "\n",
        "\n",
        "capture_work_dir = Path(work_dir_path, capture_name)\n",
        "capture_work_dir.mkdir(exist_ok=True, parents=True)\n",
        "shutil.rmtree(capture_work_dir) \n",
        "\n",
        "video_path = Path(capture_work_dir, 'raw_video')\n",
        "video_path.mkdir(exist_ok=True, parents=True)\n",
        "shutil.copy(temp_video_path, Path(video_path, video_full_name))  \n",
        "video_path = Path(video_path, video_full_name)\n",
        "\n",
        "\n",
        "# # @markdown Frames to skip.\n",
        "# skip = 1  # @param {type: 'integer'}\n",
        "skip = 1\n",
        "# @markdown Amount of frames to use from video.\n",
        "max_num_frames = 17  # @param {type: 'integer'}\n",
        "# @markdown Do center crop?\n",
        "center_crop = False  # @param {type: 'boolean'}\n",
        "# @markdown Images scale\n",
        "scale_factor = 1  #@param {type:\"slider\", min:0.1, max:1, step:0.05}\n",
        "\n",
        "\n",
        "video_to_frames = OneVideoSceneProcesser(\n",
        "    video_path=str(video_path),\n",
        "    dataset_output_path=str(capture_work_dir),\n",
        "    skip=skip,\n",
        "    center_crop=center_crop,\n",
        "    scale_factor=scale_factor,\n",
        "    max_len=max_num_frames,\n",
        "    img_prefix='jpg',\n",
        "    filter_with_sharpness=True,\n",
        ")\n",
        "\n",
        "video_to_frames.run()\n",
        "frames_path = Path(capture_work_dir, 'frames')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 游리 Open if you want to play with the matching precedure's parameters."
      ],
      "metadata": {
        "id": "lu83-TptaOkI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4dypRtZ8CLrV"
      },
      "outputs": [],
      "source": [
        "# @markdown Non Maximum Suppression (NMS) radius\n",
        "nms_radius = 4  #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "# @markdown Detector confidence threshold.\n",
        "keypoint_threshold = 0.01  #@param {type:\"slider\", min:0.001, max:0.1, step:0.001}\n",
        "# @markdown Threshold value for matching.\n",
        "match_threshold = 0.75  #@param {type:\"slider\", min:0.05, max:0.95, step:0.05}\n",
        "# @markdown Num sinkhorn iterations for super glue matching.\n",
        "sinkhorn_iterations = 20 #@param {type:\"slider\", min:1, max:50, step:1}\n",
        "# @markdown Batch size for super glue infer\n",
        "super_glue_batch = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "\n",
        "\n",
        "matcher = Matcher(\n",
        "    super_point_extractor_weights_path=superpoint_weigths_path,\n",
        "    super_glue_weights_path=superglue_weigths_path,\n",
        "    nms_radius=nms_radius,\n",
        "    keypoint_threshold=keypoint_threshold,\n",
        "    matcher_type='super_glue',\n",
        "    match_threshold=match_threshold,\n",
        "    sinkhorn_iterations=sinkhorn_iterations,\n",
        "    super_glue_batch=int(super_glue_batch)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WNhYuJdgGNok"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# @title Run sfm over images\n",
        "vs = VideoStreamer(\n",
        "    str(frames_path), \n",
        "    height=None, \n",
        "    width=None, \n",
        "    max_len=None, \n",
        "    img_glob='*.jpg'\n",
        "  )\n",
        "camera_size = vs.get_resolution()\n",
        "\n",
        "colmap = ColmapBdManager(\n",
        "    db_dir=str(Path(capture_work_dir, 'colmap')),\n",
        "    images_folder_path=str(frames_path),\n",
        "    camera_type='OPENCV',\n",
        "    camera_params=None,\n",
        "    camera_size=camera_size\n",
        ")\n",
        "\n",
        "processed_frames, match_table, images_names = matcher.match_video_stream(vs, lambda x: True)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "colmap.replace_images_data(images_names)\n",
        "colmap.replace_keypoints(images_names, processed_frames)\n",
        "colmap.replace_and_verificate_matches(match_table, images_names)\n",
        "num_sparse_points = colmap.run_mapper()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GpoXkZGPT5L"
      },
      "source": [
        "## 3. Selecting source view for building MLI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 游리 Some scenes have objects of interest too near or too far. You can adjust **translation_scale** for the best result on your scene. However, the default value was OK for most scenes that we tried.\n"
      ],
      "metadata": {
        "id": "T7GNARGkWCo3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5dCYq3zGts7V"
      },
      "outputs": [],
      "source": [
        "colmap_sparse_to_re10k_like_views(\n",
        "    scene_colmap_sparse_path=os.path.join(capture_work_dir, 'colmap', 'sparse'),\n",
        "    views_file_output_path=capture_work_dir,\n",
        "    scene_meta_file_output_path=capture_work_dir,\n",
        ")\n",
        "\n",
        "# # @markdown Source camera resolution rescale:\n",
        "# source_camera_scale = 1  #@param {type:\"slider\", min:0.1, max:1, step:0.05}\n",
        "\n",
        "source_camera_scale = 1\n",
        "\n",
        "# @markdown Scene scale:\n",
        "translation_scale = 1.6 #@param {type:\"slider\", min:1, max:5, step:0.1}\n",
        "\n",
        "resize_size = [int(camera_size[1] * source_camera_scale),\n",
        "             int(camera_size[0] * source_camera_scale)] \n",
        "\n",
        "\n",
        "# resize_size = [math.floor(resize_size[0] / 16) * 16,\n",
        "#                math.floor(resize_size[1] / 16) * 16] \n",
        "\n",
        "\n",
        "crop_size = [math.floor(resize_size[0] / 16) * 16,\n",
        "             math.floor(resize_size[1] / 16) * 16] \n",
        "\n",
        "intrinsics, extrinsics, images = read_re10k_views(\n",
        "    views_file_path=os.path.join(capture_work_dir, 'views.txt'),\n",
        "    scene_meta_path=os.path.join(capture_work_dir, 'scene_meta.yaml'),\n",
        "    frames_path=os.path.join(capture_work_dir, 'frames'),\n",
        "    frames_crop_size=crop_size,\n",
        "    frames_resize_size=resize_size,\n",
        "    translation_scale=translation_scale\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "extrinsics = torch.stack(extrinsics, dim=0).cuda()\n",
        "intrinsics = torch.stack(intrinsics, dim=0).cuda()\n",
        "images = torch.stack(images, dim=0).cuda()\n",
        "source_images_sizes = images.shape[-2:]\n",
        "\n",
        "all_cameras = CameraMultiple(extrinsics=extrinsics, \n",
        "                             intrinsics=intrinsics,\n",
        "                             images_sizes=[list(images.shape[-2:])] * extrinsics.shape[0]\n",
        "                            )\n",
        "\n",
        "# @markdown Select number of views for bulding MLI\n",
        "num_views = 8  #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "def find_most_distant_point_a_to_b(set_a, set_b):\n",
        "  size_a = set_a.shape[0]\n",
        "  size_b = set_b.shape[0]\n",
        "  distances = torch.sqrt(torch.sum((set_b.repeat(size_a, 1) - set_a.repeat(1, size_b).reshape(-1, 3))**2, dim=1))\n",
        "  min_dist, _ = distances.reshape(size_a, -1).min(dim=1)\n",
        "  return torch.argmax(min_dist)\n",
        "\n",
        "def find_most_distant_point(point_set):\n",
        "  size_point_set = point_set.shape[0]\n",
        "  distances = torch.sqrt(torch.sum((point_set.repeat(size_point_set, 1) - point_set.repeat(1, size_point_set).reshape(-1, 3))**2, dim=1))\n",
        "  mask = (distances == 0)\n",
        "  distances = distances + mask * max(distances)\n",
        "  min_dist, _ = distances.reshape(size_point_set, -1).min(dim=1)\n",
        "  return torch.argmax(min_dist)\n",
        "\n",
        "def get_k_most_distant_cams(cameras, k):\n",
        "  cam_pos = cameras.world_position\n",
        "  cam_dir = cameras.world_view_direction()\n",
        "\n",
        "  first_point_id = find_most_distant_point(cam_pos)\n",
        "  result = cam_pos[[first_point_id]][None]\n",
        "  result_id = first_point_id[None]\n",
        "\n",
        "  k = num_views\n",
        "  for i in range(k - 1):\n",
        "    next_point_id = find_most_distant_point_a_to_b(cam_pos, cam_pos[result_id])\n",
        "    result_id = torch.cat([result_id, next_point_id[None]])\n",
        "\n",
        "  return cameras[result_id], result_id\n",
        "\n",
        "selected_cameras, select_id = get_k_most_distant_cams(all_cameras, num_views)\n",
        "selected_images = images[select_id]\n",
        "\n",
        "# selected_cameras = all_cameras\n",
        "# selected_images = images\n",
        "\n",
        "reference_extrinsic = average_extrinsics(selected_cameras.extrinsics)\n",
        "\n",
        "# # @markdown Reference camera scale\n",
        "# ref_camera_scale = 2  #@param {type:\"slider\", min:0.5, max:2, step:0.05}\n",
        "\n",
        "ref_camera_scale = 1\n",
        "ref_intrinsic = intrinsics[:1].clone()\n",
        "reference_images_sizes = [int(source_images_sizes[0] * ref_camera_scale),\n",
        "                          int(source_images_sizes[1] * ref_camera_scale)]\n",
        "reference_images_sizes = [reference_images_sizes[0] // 16 * 16, reference_images_sizes[1] // 16 * 16]\n",
        "ref_intrinsic[:, :2, :2] = ref_intrinsic[:, :2, :2] / ref_camera_scale\n",
        "\n",
        "\n",
        "# reference_images_sizes = [720, 1280]\n",
        "# reference_images_sizes = [reference_images_sizes[0] // 16 * 16, reference_images_sizes[1] // 16 * 16]\n",
        "# ref_intrinsic = torch.tensor([[[reference_images_sizes[0] / reference_images_sizes[1], 0, 0.5],\n",
        "#                                [0, 1.0, 0.5],\n",
        "#                                [0, 0, 1.0]]]).cuda()\n",
        "\n",
        "reference_camera = CameraMultiple(extrinsics=reference_extrinsic[None, None],\n",
        "                                  intrinsics=ref_intrinsic[None, None],\n",
        "                                  images_sizes=reference_images_sizes,\n",
        "                                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Viewer of scene camera poses \n",
        "#### 游리 Open this section to see a plot of scene cameras and which cameras were selected for building MLI (orange points). :\n",
        "#### 游리 The green arrow shows a camera view direction, the blue arrow indicates a camera UP direction.\n",
        "#### 游리 The green point is a virtual camera in which a frustum MLI representation will be built."
      ],
      "metadata": {
        "id": "XSeq2hq0LNtH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBfI3XdlXfAP",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ___\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "# @markdown Plot scale:\n",
        "plot_size = 1  #@param {type:\"slider\", min:0.01, max:1, step:0.01}\n",
        "\n",
        "def ploty_plot_extrinsics(fig, extrinsics, axis_scale=0.01, line_width=4, marker_size=1, opacity=1):\n",
        "    translation = extrinsics[..., :3, -1:]\n",
        "    rotation = extrinsics[..., :3, :3]\n",
        "\n",
        "    cameras_world_positions = -rotation.transpose(-1, -2) @ translation\n",
        "    cameras_world_positions = cameras_world_positions[:, :, 0]\n",
        "    cameras_R = rotation\n",
        "    \n",
        "    fig = fig.add_trace(go.Scatter3d(\n",
        "                    x=cameras_world_positions[:, 0], \n",
        "                    y=cameras_world_positions[:, 1], \n",
        "                    z=cameras_world_positions[:, 2],\n",
        "                    mode=\"markers+text\",\n",
        "                    marker={\"size\": marker_size, 'color':'black'},\n",
        "                    text=[str(el) for el in range(cameras_R.shape[0])],\n",
        "                    textposition=\"middle left\",\n",
        "                    opacity=opacity\n",
        "                    ))\n",
        "    \n",
        "    for i in range(cameras_R.shape[0]):\n",
        "        R, T = cameras_R[i], cameras_world_positions[i]\n",
        "\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=[T[0], T[0] + R[0, 0] * axis_scale], \n",
        "            y=[T[1], T[1] + R[0, 1] * axis_scale], \n",
        "            z=[T[2], T[2] + R[0, 2] * axis_scale], \n",
        "            line={\"color\":'red', 'width':line_width},\n",
        "            mode='lines', \n",
        "            showlegend=False,\n",
        "            opacity=opacity\n",
        "        ))\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=[T[0], T[0] + R[1, 0] * axis_scale], \n",
        "            y=[T[1], T[1] + R[1, 1] * axis_scale], \n",
        "            z=[T[2], T[2] + R[1, 2] * axis_scale], \n",
        "            line={\"color\":'green', 'width':line_width},\n",
        "            mode='lines', \n",
        "            showlegend=False,\n",
        "            opacity=opacity\n",
        "        ))\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=[T[0], T[0] + R[2, 0] * axis_scale], \n",
        "            y=[T[1], T[1] + R[2, 1] * axis_scale], \n",
        "            z=[T[2], T[2] + R[2, 2] * axis_scale], \n",
        "            line={\"color\":'blue', 'width':line_width},\n",
        "            mode='lines', \n",
        "            showlegend=False,\n",
        "            opacity=opacity\n",
        "        ))\n",
        "        \n",
        "    \n",
        "    return fig\n",
        "\n",
        "  \n",
        "\n",
        "def configure_plotly_browser_state():\n",
        "  import IPython\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))\n",
        "\n",
        "all_cam_pos = all_cameras.world_position.cpu()\n",
        "cams_positions = go.Scatter3d(\n",
        "    x=all_cam_pos[:, 0].numpy(), \n",
        "    y=all_cam_pos[:, 1].numpy(), \n",
        "    z=all_cam_pos[:, 2].numpy(),\n",
        "    mode='markers', \n",
        "    surfacecolor='#ff0000',\n",
        "    name='not selected'\n",
        "    )\n",
        "\n",
        "selected_cam_pos = selected_cameras.world_position.cpu()\n",
        "cams_distant = go.Scatter3d(\n",
        "    x=selected_cam_pos[:, 0].numpy(), \n",
        "    y=selected_cam_pos[:, 1].numpy(), \n",
        "    z=selected_cam_pos[:, 2].numpy(),\n",
        "    mode='markers', \n",
        "    surfacecolor='#0000ff',\n",
        "    name='selected'\n",
        "    )\n",
        "\n",
        "ref_cam_pos = reference_camera.world_position.cpu()\n",
        "cams_ref = go.Scatter3d(\n",
        "    x=ref_cam_pos[0, 0, :, 0].numpy(), \n",
        "    y=ref_cam_pos[0, 0, :, 1].numpy(), \n",
        "    z=ref_cam_pos[0, 0, :, 2].numpy(),\n",
        "    mode='markers', \n",
        "    surfacecolor='#00ffff',\n",
        "    name='reference camera'\n",
        "    )\n",
        "\n",
        "fig = go.Figure(data=[cams_positions, cams_distant, cams_ref])\n",
        "\n",
        "fig = ploty_plot_extrinsics(fig, all_cameras.extrinsics.cpu(), opacity=0.5, axis_scale=0.06)\n",
        "\n",
        "fig.update_layout(\n",
        "    scene = dict(\n",
        "        xaxis = dict(nticks=4, range=[-plot_size, plot_size],),\n",
        "        yaxis = dict(nticks=4, range=[-plot_size, plot_size],),\n",
        "        zaxis = dict(nticks=4, range=[-plot_size, plot_size],),\n",
        "    ),\n",
        ")\n",
        "\n",
        "fig.update_layout(scene_aspectmode='cube')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbvRd7vwPbjz"
      },
      "source": [
        "## 4. Build MLI with the SIMPLI model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-2Mtj-9n6iqV"
      },
      "outputs": [],
      "source": [
        "# @title Run SIMPLI \n",
        "%%time\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "trainer.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  result = trainer.gen.manual_forward(source_images=selected_images[None], \n",
        "                                      source_cameras=selected_cameras[None, None],\n",
        "                                      reference_cameras=reference_camera\n",
        "                                      )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY1o1ukTWPiO"
      },
      "outputs": [],
      "source": [
        "! rm -rf '/usr/local/share/jupyter/nbextensions/google.colab/multi-layer-viewer/mli_scene'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7ednwgElYNaW"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "#@title Save MLI\n",
        "def save_mpi(mpi, path, save_as_jpg=False):\n",
        "    n_planes, channels, height, width = mpi.shape\n",
        "    mpi = np.concatenate([mpi[:, :-1] * 0.5 + 0.5, mpi[:, -1:]], axis=1)\n",
        "    mpi = mpi.transpose((0, 2, 3, 1))\n",
        "    mpi = (255 * np.clip(mpi, 0, 1)).astype(np.uint8)\n",
        "\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    for i, layer in enumerate(mpi):\n",
        "        if save_as_jpg:\n",
        "          Image.fromarray(layer[:, :, :3]).save(os.path.join(path, f'layer_{i:02d}.jpg'), optimize=True)\n",
        "          Image.fromarray(np.repeat(layer[:, :, -1:], 3, axis=2)).save(os.path.join(path, f'layer_alpha_{i:02d}.jpg'), optimize=True)\n",
        "        else:\n",
        "            Image.fromarray(layer).save(os.path.join(path, f'layer_{i:02d}.png'), optimize=True)\n",
        "\n",
        "def save_layered_depth(layered_depth, path):\n",
        "  num_layers, h, w = layered_depth.shape\n",
        "  depth_meta_data = {}\n",
        "  for i in range(num_layers):\n",
        "      depth = layered_depth[i]\n",
        "      low, high = np.min(depth), np.max(depth)\n",
        "      scaled = (depth - low) / (high - low)\n",
        "      ui = np.clip(scaled * 256.0, 0, 255).astype(np.uint8)\n",
        "      img = Image.fromarray(ui)\n",
        "\n",
        "      tag = f'layer_depth_{i:02d}'\n",
        "      img.save(os.path.join(path, tag + '.jpg'), quality=100)\n",
        "      depth_meta_data[tag] = [low.item(), high.item()]\n",
        "\n",
        "  return depth_meta_data\n",
        "\n",
        "def write_meta(depth_meta_data, resolution, extrinsic, intrinsic, path):\n",
        "      meta_data = depth_meta_data\n",
        "\n",
        "      meta_data.update({'frame_size': resolution,\n",
        "                        'extrinsic_re':extrinsic,\n",
        "                        'intrinsics_re':intrinsic,\n",
        "                        })\n",
        "\n",
        "      with open(os.path.join(path,'meta.json'), 'w') as f:\n",
        "        json.dump(meta_data, f, ensure_ascii=True)\n",
        "\n",
        "geom_path = '/usr/local/share/jupyter/nbextensions/google.colab/multi-layer-viewer/mli_scene/'\n",
        "\n",
        "mpi = result['mpi'][0, 0].cpu()\n",
        "layered_depth = result['layered_depth'][0].cpu()\n",
        "\n",
        "save_mpi(mpi, geom_path, save_as_jpg=True)\n",
        "depth_meta_data = save_layered_depth(layered_depth.numpy(), geom_path)\n",
        "\n",
        "intr = reference_camera.intrinsics[0].flatten().cpu().numpy()\n",
        "extr = reference_camera.extrinsics[0].flatten().cpu().numpy()\n",
        "write_meta(depth_meta_data, \n",
        "           [layered_depth.shape[1], layered_depth.shape[2]], \n",
        "           list(extr.astype(float)), \n",
        "           list(intr.astype(float)), \n",
        "           geom_path)\n",
        "\n",
        "final_mli_path = os.path.join(capture_work_dir, f'{capture_name}_mli.zip')\n",
        "final_geom_path = os.path.join(geom_path, '*')\n",
        "! zip -9jpr {final_mli_path} {final_geom_path}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay_XrfudRM6J"
      },
      "outputs": [],
      "source": [
        "print(f'You can find saved MLI here :{final_mli_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLI viewer"
      ],
      "metadata": {
        "id": "A4BtycBOBr2o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nEjf5PXEGvza"
      },
      "outputs": [],
      "source": [
        "#@title ___\n",
        "import IPython\n",
        "display(IPython.core.display.HTML(f'''\n",
        "      <head>\n",
        "    <title>Multi-Layer Viewer</title>\n",
        "    <link rel=\"stylesheet\" href=\"/nbextensions/google.colab/multi-layer-viewer/viewer.css\">\n",
        "  </head>\n",
        "\n",
        "   <head>\n",
        "    <title>Multi-Layer Viewer</title>\n",
        "    <link rel=\"stylesheet\" href=\"/nbextensions/google.colab/multi-layer-viewer/viewer/viewer.css\">\n",
        "\n",
        "    <script src=\"/nbextensions/google.colab/multi-layer-viewer/threejs_bin.js\"></script>\n",
        "    <script src=\"/nbextensions/google.colab/multi-layer-viewer/papaparse.min.js\"></script>\n",
        "    <script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js\"> </script>\n",
        "    <script src=\"https://mrdoob.github.io/stats.js/build/stats.min.js\"></script>\n",
        "    <script src=\"/nbextensions/google.colab/multi-layer-viewer/numjs.min.js\"></script>\n",
        "    <script src=\"/nbextensions/google.colab/multi-layer-viewer/utils.js\"></script>\n",
        "    <script src=\"/nbextensions/google.colab/multi-layer-viewer/camera_viewer.js\"></script>\n",
        "    <script src=\"/nbextensions/google.colab/multi-layer-viewer/materials.js\"></script>\n",
        "    <script src=\"/nbextensions/google.colab/multi-layer-viewer/viewer.js\"></script>\n",
        "\n",
        "  </head>\n",
        "\n",
        "   <body>\n",
        "    <div class=\"container\">\n",
        "\n",
        "      <div class=\"title\" id=\"title\">\n",
        "        <h1>Multi-Layer Viewer</h1>\n",
        "      </div>\n",
        "\n",
        "<div class=\"main_controls\">\n",
        "      <div class=\"controls\" style=\"padding-top: 20px;\">\n",
        "        <div><p style=\"font-size: 13pt; line-height: 0.05\">Movement mode:</p></div>\n",
        "        <div class=\"column_\">\n",
        "             <button id=\"hoverButton\">Hover</button>\n",
        "             <div style=\"width: 10px; height: 3px\"></div>\n",
        "             <button id=\"dragButton\">Drag</button>\n",
        "             <div style=\"width: 10px; height: 3px\"></div>\n",
        "             <button id=\"swayButton\">Sway</button>\n",
        "             <div style=\"width: 10px; height: 3px\"></div>\n",
        "             <button id=\"wonderButton\">Wonder</button>\n",
        "         </div>\n",
        "      </div>\n",
        "\n",
        "      <div class=\"controls\" style=\"padding-top: 30px;\">\n",
        "        <div><p style=\"font-size: 13pt; line-height: 0.05\">View mode:</p></div>\n",
        "        <div class=\"column_\">\n",
        "            <button id=\"magmaViewButton\">Depth (Magma)</button>\n",
        "            <div style=\"width: 10px; height: 3px\"></div>\n",
        "            <button id=\"rainbowViewButton\">Depth (Rainbow)</button>\n",
        "            <div style=\"width: 10px; height: 3px\"></div>\n",
        "            <button id=\"normalViewButton\">Normal</button>\n",
        "        </div>\n",
        "      </div>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "        <div class=\"main-viewer\" style=\"padding-left: 23%; margin-top: -29%;\">\n",
        "           <div class=\"viewer-container\">\n",
        "        <div id=\"scene-viewer\" class=\"scene-lf\">\n",
        "         <canvas id=\"viewer-canvas\" class=\"viewer-canvas\" width=\"994\" height=\"596\">\n",
        "            <param id=\"base-path\" value=\"../scenes/3/\">\n",
        "          <div id=\"display\"></div>\n",
        "                 <script>\n",
        "                document.getElementById(\"display\").innerHTML = startDisplay(\"/nbextensions/google.colab/multi-layer-viewer/mli_scene\", num_layers={num_layers});\n",
        "              </script>\n",
        "\n",
        "         </canvas>\n",
        "        </div>\n",
        "      </div>\n",
        "      </div>\n",
        "\n",
        "    </div>\n",
        "\n",
        "  </body>\n",
        "      '''))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}